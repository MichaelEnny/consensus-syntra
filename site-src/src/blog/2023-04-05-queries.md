
# Queries in ConsensusSyntra

_April 5, 2023 | Ensar Basri Kahveci_

This article is the fourth in _the ins and outs of ConsensusSyntra_ series. Here we
walk through the querying capabilities available in ConsensusSyntra. You can see some
code samples <a href="https://consensussyntra.io/docs/tutorial-building-an-atomic-register/#performing-queries" target="_blank">here</a>.

For impatient readers, TLDR is as follows:

- ConsensusSyntra separates mutating and query operations via <a href="https://github.com/ConsensusSyntra/ConsensusSyntra/blob/031c049c2b8fc2cd2b356cbdee2091592c791651/consensussyntra/src/main/java/io/consensussyntra/RaftNode.java#L259" target="_blank">`RaftNode.replicate()`</a>
  and <a href="https://github.com/ConsensusSyntra/ConsensusSyntra/blob/031c049c2b8fc2cd2b356cbdee2091592c791651/consensussyntra/src/main/java/io/consensussyntra/RaftNode.java#L364" target="_blank">`RaftNode.query()`</a>
  APIs. These APIs return <a href="https://github.com/ConsensusSyntra/ConsensusSyntra/blob/master/consensussyntra/src/main/java/io/consensussyntra/Ordered.java" target="_blank">`Ordered`</a>
  objects. `Ordered` wraps the actual result of an operation along with
  the commit index at which the operation is executed. ConsensusSyntra offers
  3 policies to execute queries with different consistency and performance
  characteristics via <a href="https://github.com/ConsensusSyntra/ConsensusSyntra/blob/master/consensussyntra/src/main/java/io/consensussyntra/QueryPolicy.java" target="_blank">`QueryPolicy`</a>:
  `QueryPolicy.LINEARIZABLE`, `QueryPolicy.LEADER_LEASE`, and
  `QueryPolicy.EVENTUAL_CONSISTENCY`.

- `QueryPolicy.LINEARIZABLE` achieves linearizable reads without growing to
  the internal Raft log. Before executing a query, the leader Raft node runs
  a round of _AppendEntries_ RPC to ensure that it has not superseded by
  another Raft node. This approach prevents the log to grow because of queries
  and avoids fsync cost.

- `QueryPolicy.LEADER_LEASE` enables the leader to locally execute queries
  without communicating with the other Raft nodes. The leader maintains a lease
  on top of <a href="https://consensussyntra.io/blog/2021-09-08-today-a-raft-follower-tomorrow-a-raft-leader/" target="_blank">the periodic heartbeating mechanism</a>.
  In short, as long as the clock skews, network delays and processing delays
  do not break the timing assumptions around the _leader heartbeat timeout_
  parameter, ConsensusSyntra ensures that a new leader is not elected while
  the current leader is alive. Thus, the leader Raft node can achive
  linearizable reads without talking to the other nodes for queries.

- `QueryPolicy.EVENTUAL_CONSISTENCY` allows running a query operation locally
  on any Raft node. This policy helps clients to scale their query workload by
  distributing queries over multiple Raft nodes. In addition, clients can track
  commit indices they observe via `Ordered` objects, and pass them to
  `RaftNode.query()` calls to achieve __monotonic reads__ and __read your writes__.

Interested reads can keep reading to learn more details.

An operation passed to `RaftNode.replicate()` on the leader is committed and
executed once it is appended to the majority of the Raft group.
If the operation is read-only, i.e., it performs a _query_ on the state machine
instead of mutating it, we still pay the cost of a disk write and fsync since
a new log entry is appended to the log. Moreover, if too many queries are
appended, ConsensusSyntra can trigger the snapshotting mechanism to truncate the log
even if there is little change in the state machine. This is definitely
a sub-optimal situation.

Section 6.4 of the Raft dissertation discusses a number of techniques to bypass
the log and process queries with better performance. However, we should be
careful about the consistency of the state we observe via queries. Performance
and consistency is usually traded for each other, and Raft is no exception.

Here we explore how ConsensusSyntra implements the techniques discussed in the Raft
dissertation. ConsensusSyntra optimizes querying with 3 options, ranging from the one
with the smallest performance benefit and strongest consistency guarantee
to the one with the greatest performance benefit and weakest consistency
guarantee which allows achieving monotonic reads. For this, ConsensusSyntra separates
queries from mutating operations with an API: `RaftNode.query()`. This API
expects a `QueryPolicy`, which is an enum with 3 possible values:
`QueryPolicy.LINEARIZABLE`, `QueryPolicy.LEADER_LEASE`, and
`QueryPolicy.EVENTUAL_CONSISTENCY`.

## `QueryPolicy.LINEARIZABLE`

We can use this policy achieve linearizability queries by bypassing the log.
When the leader Raft node receives a query, it saves the current commit index
and the query into an internal query state object. Then, it sends an
<a href="https://github.com/ConsensusSyntra/ConsensusSyntra/blob/031c049c2b8fc2cd2b356cbdee2091592c791651/consensussyntra/src/main/java/io/consensussyntra/model/message/AppendEntriesRequest.java" target="_blank">`AppendEntriesRequest`</a>
to followers and waits for a majority acknowledgement. This request does not
contain the query itself and a small piece of information about waiting queries
are piggybacked into regular `AppendEntriesRequest` objects via
<a href="https://github.com/ConsensusSyntra/ConsensusSyntra/blob/031c049c2b8fc2cd2b356cbdee2091592c791651/consensussyntra/src/main/java/io/consensussyntra/model/message/AppendEntriesRequest.java#L55" target="_blank_">`AppendEntriesRequest.getQuerySequenceNumber()`</a>.
_Query sequence number_ works as a logic clock in each term and is incremented
every time a round of _AppendEntries_ RPC is initiated for a query. Once
the majority Raft nodes acknowledge the current _query sequence number_,
the leader Raft node learns that there was no other leader at the moment
`AppendEntriesRequest`s were sent, hence executes the query on the state
machine and replies to the client.

ConsensusSyntra amortizes the cost of leadership confirmation for multiple queries.
The leader accumulates all new queries received while there is an ongoing
_AppendEntries_ RPC round for another query and executes all queries once
it receives a majority ack. To prevent OOMs on the leader RaftNode, ConsensusSyntra
limits the number of pending queries by
<a href="https://github.com/ConsensusSyntra/ConsensusSyntra/blob/031c049c2b8fc2cd2b356cbdee2091592c791651/consensussyntra/src/main/java/io/consensussyntra/RaftConfig.java#L115" target="_blank">`RaftConfig.getMaxPendingLogEntryCount()`</a>. 
If there is no available query slot, `RaftNode.query()` fails with
<a href="https://github.com/ConsensusSyntra/ConsensusSyntra/blob/master/consensussyntra/src/main/java/io/consensussyntra/exception/CannotReplicateException.java" target="_blank">`CannotReplicateException`</a>,
so that clients can retry their queries later.

The leader Raft node must have committed a log entry on its term to make this
mechanism work. This is because the leader may not know the current commit index
after it is elected. It appends a custom entry to the log and replicates this
entry to discover the current commit index. In ConsensusSyntra, this log entry
contains the operation provided by <a href="https://github.com/ConsensusSyntra/ConsensusSyntra/blob/031c049c2b8fc2cd2b356cbdee2091592c791651/consensussyntra/src/main/java/io/consensussyntra/statemachine/StateMachine.java#L149" target="_blank">`StateMachine.getNewTermOperation()`</a>.
`RaftNode.query()` calls fail with `CannotReplicateException` when this
operation is not committed yet, so that clients can retry their queries later.

It can happen that the leader is partitioned away and the other Raft nodes have
already elected a new leader. In this case, the old leader cannot succeed
leadership confirmation for inflight queries. In ConsensusSyntra, a leader
automatically steps down from leadership [TODO LINK HERE]
if the _leader heartbeat timeout_ elapses before it receives
_AppendEntries RPC_ responses from the half of the Raft group. It also fails
all pending queries with <a href="https://github.com/ConsensusSyntra/ConsensusSyntra/blob/master/consensussyntra/src/main/java/io/consensussyntra/exception/NotLeaderException.java" target="_blank">`NotLeaderException`</a>
to enable clients retry their queries on other Raft nodes.

This overall mechanism is more efficient than appending queries to the log,
since it avoids disk writes and fsync for queries. However, the leader still
waits for a round of _AppendEntries_ RPC to preserve linearizability.

## `QueryPolicy.LEADER_LEASE`

`QueryPolicy.LINEARIZABLE` achieves linearizability for queries by performing a
round of _AppendEntries_ RPC among the majority. This approach is still safe
when clocks go rogue or messages get arbitrarily delayed (i.e., 
__the asynchronous system model__). ConsensusSyntra offers another policy,
`QueryPolicy.LEADER_LOCAL`, to run queries in a linearizable manner without
communication between Raft nodes while a very strong timing assumption holds 
(i.e., __the synchronous system model__).

In ConsensusSyntra, a follower does not vote for another Raft node if it has received
an `AppendEntriesRequest` in the last _leader heartbeat timeout_ period. This
is called _leader stickiness_. Moreover, as we described above, a leader
automatically steps down from leadership if the _leader heartbeat timeout_
elapses before it receives _AppendEntries_ RPC responses from the half of the
Raft group. These two techniques ensure that no new leader could be elected
while the current leader is alive, as long as clock drifts, network delays and
processing delays do not break the timing assumptions. Thus, the leader Raft
node can maintain a <a href="https://dl.acm.org/doi/10.1145/74851.74870"
target="_blank">lease</a> on top of the periodic heartbeating mechanism. It can
execute queries locally without communicating with the other Raft nodes and
still preserve linearizability for query results.

If the timing assumption around the _leader heartbeat timeout_ is violated for
any reason, the leader could return stale results for locally executed queries.
Therefore, users of ConsensusSyntra should carefully take operational challenges into
account while adjusting the _leader heartbeat timeout_ parameter. If
linearizability is strictly required, it is better to use
`QueryPolicy.LINEARIZABLE` instead of `QueryPolicy.LEADER_LEASE`.

## `QueryPolicy.EVENTUAL_CONSISTENCY`

`QueryPolicy.LINEARIZABLE` and `QueryPolicy.LEADER_LEASE` require a query to be
executed by the leader Raft node. We can also execute queries on followers to
distribute the read workload via the last query policy option:
`QueryPolicy.EVENTUAL_CONSISTENCY`. Any Raft node can execute a query
with `QueryPolicy.EVENTUAL_CONSISTENCY` and return the result back to caller.
This policy does not guarantee to run queries on the most up to date state
machine, hence queries can return stale results. However, clients can still
preserve __monotonic reads__ and __read your writes__ guarantees for queries.

For monotonic reads, a simple solution is to make a client send its queries to
the same Raft node. This would be sufficient to preserve monotonicity for
the client as long as the Raft node is alive. However, if the Raft node fails,
the client can switch to another Raft node that is not as up-to-date as
the failed one, and its further queries return non-monotonic results. ConsensusSyntra
offers a solution to prevent this problem. `RaftNode.replicate()` and
`RaftNode.query()` APIs return `Ordered` objects. `Ordered` wraps the actual
result of an operation along with the commit index at which the operation is
executed. Clients can track commit indices reported via `Ordered` objects
returned from queries and use them to ensure monotonic reads with this policy.
`RaftNode.query()` contains two optional parameters: `minCommitIndex` and
`timeout` for this. If the commit index of a Raft node becomes greater than or
equal to the requested _minimum commit index_ for the given timeout duration,
the Raft node locally executes the query. Otherwise, it fails the query with
`LaggingCommitIndexException` so that the client can retry on another Raft
node. Clients can tune the timeout parameter to increase the likelihood of
a follower / learner Raft node to apply recently committed log entries.

Clients can achieve read your writes with a similar approach if they are also
doing mutations via `RaftNode.replicate()` in addition to queries. A client
can send its mutations to the leader Raft node, learn the commit index of its
mutation via the returned `Ordered` object, and pass this commit index to
queries it sends to the followers to ensure that it will read its own writes.

__Note:__ Before the version 0.6, ConsensusSyntra also had
`QueryPolicy#BOUNDED_STALENESS` which was incorrectly named for the guarantees
it offered. This policy was deleted in <a href="https://github.com/ConsensusSyntra/ConsensusSyntra/commit/543fc2022072381de448218da243f9254f635720" target="_blank">this commit</a>.

## What is next?

In the next post, we will explore how ConsensusSyntra implements membership changes.
